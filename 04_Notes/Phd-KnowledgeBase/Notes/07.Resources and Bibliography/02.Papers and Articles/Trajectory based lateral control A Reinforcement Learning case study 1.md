---
Reference ID: A unique identifier for each entry.
Title: Title of the paper/article/book
Authors: Authors of the work.
Year: Year of publication.
Source: Journal name, conference, or publisher.
Keywords:
  - Keywords relevant to your research.
  - this is another row
Methodology: The methodology used in the paper (experimental, review, simulation, etc.).
Summary: Your brief summary of the paper.
Link/DOI: Direct link or DOI for quick access.
Cited By: How many times it has been cited (you can get this from Google Scholar, for instance). It helps in identifying influential papers.
Relevance: High/Medium/Low
tags:
  - "#research/paper"
  - research/communication
File Path/Link: If you have a digital copy, link to its location on your drive for quick access.
Date Reviewed: 
Status: Open/In Progress/Done
---


## Reflections/Notes

**Key Objectives/Questions:**
Example: To explore the challenges and opportunities of implementing POMDPs in real-world multi-agent driving scenarios.

**Main Findings/Results:**
1. POMDPs present challenges in multi-agent environments due to X, Y, Z.
2. Proposed solution A improved computational efficiency by 20%.  

**Methodologies Used:**
Example: The authors used a hybrid approach combining traditional POMDPs with a new algorithm for faster computation.

**Relevance to My Research:**
Example: This paper's approach to improving computational efficiency in POMDPs can be beneficial for my own work on real-time decision making in autonomous vehicles.

**Key Quotations:**
Example: "Our findings suggest that while traditional POMDPs struggle in complex environments, hybrid solutions present a promising direction for future research."

**Personal Reflections/Thoughts:**
Example: While the paper presented a novel approach, there are still some questions about the scalability of their proposed solution. It would be interesting to experiment with their algorithm in a more diverse set of scenarios.
  

## Actionable Next Steps:

Task list:
1. [ ] Replicate the hybrid algorithm discussed in the paper.
2. [ ] Compare its efficiency with the current model I'm working on.


|   |   |
|---|---|
|**Connected papers**|LINK|
|**MindManager Link**|LINK|
|**File Link**|[LINK](file:///D:\03_PHD\_github\Phd--POMDP\02_Resources\articles\Engineering_Applications_of_Artificial_Intelligence_2020_sep_vol.pdf)|
|**Article Note Link**|[Trajectory based lateral control: A Reinforcement Learning case study](Trajectory%20based%20lateral%20control%20A%20Reinforcement%20Learning%20case%20study.md)|
|Arxiv|Paste Ref￼<br><br>Asanka Wasala 1, Donal Byrne 2, Philip Miesbauer, Joseph O’Hanlon, Paul Heraty 2, Peter Barry<br><br>  <br><br>Reinforcement Learning (RL) has been employed in many applications of robotics and has steadily been gaining traction in the field of [Autonomous Driving](https://www.sciencedirect.com/topics/computer-science/autonomous-driving) (AD). This paper proposes a [Deep Reinforcement Learning](https://www.sciencedirect.com/topics/computer-science/deep-reinforcement-learning) based approach for lateral Vehicle Motion Control (VMC), and explores the [generalization](https://www.sciencedirect.com/topics/computer-science/generalization) capabilities of the approach. The proposed methodology uses a sequence of waypoints generated from a planning module of an AD stack as the input. The network has been trained to predict accurate steering commands to follow the given trajectory. In this paper we detail our implementation and share our [learning experience](https://www.sciencedirect.com/topics/computer-science/learning-experiences) on real-vehicle deployment of the RL based controller. Our experiments yield promising results with an agent trained on less than 4 h of simulated driving experience without any real-world data. The trained agent is able to successfully complete unseen and more complex tracks using different unseen vehicle models. The agent safely reached up to 150 km/h in simulation and up to 60 km/h in a real-life [Sport Utility Vehicle](https://www.sciencedirect.com/topics/engineering/sport-utility-vehicle) (SUV) weighing more than 2000 kg.<br><br>  <br>> From <[https://www.sciencedirect.com/science/article/abs/pii/S0952197620301858?via%3Dihub](https://www.sciencedirect.com/science/article/abs/pii/S0952197620301858?via%3Dihub)>  <br><br>Trajectory based lateral control: A Reinforcement Learning case study<br><br>- [https://doi.org/10.1016/j.engappai.2020.103799](https://doi.org/10.1016/j.engappai.2020.103799)<br><br>  <br>  <br>> From <[https://www.sciencedirect.com/science/article/abs/pii/S0952197620301858?via%3Dihub](https://www.sciencedirect.com/science/article/abs/pii/S0952197620301858?via%3Dihub)>|