{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 - Introduction to PyTorch (Solution)\n",
    "\n",
    "In this exercise, we will introduce the fundamental concepts of PyTorch and show how this framework can be applied to build and train custom neural networks. \n",
    "\n",
    "\n",
    "**What is PyTorch?**\\\n",
    "PyTorch is a Python-based library designed for deep learning. It is distinguished by its dynamic computational graph, which enables researchers and developers to construct models with a high degree of flexibility. PyTorch has found extensive use in various scientific and engineering domains due to its ease of use and extensive research-friendly features.\n",
    "\n",
    "This exercise is based on the [PyTorch 60-Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Tensors\n",
    "Tensors are used to encode inputs and outputs of a model, as well as a model's parameters. They are comparable to NumPy's ndarrays, but they have the advantage that PyTorch tensors can run on GPUs and other accelerators. \n",
    "\n",
    "Extra ressources: [Intuition behind tensors](https://www.youtube.com/watch?v=f5liqUk0ZTw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1.1 Tensor Initialization**\n",
    "\n",
    "Tensors can be initialized in multiple ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor from data: \n",
      " tensor([[1, 2],\n",
      "        [3, 4]]) \n",
      "\n",
      "Tensor from NumPy array: \n",
      " tensor([[1, 2],\n",
      "        [3, 4]]) \n",
      "\n",
      "Shape given by another tensor:\n",
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.4235, 0.8584],\n",
      "        [0.8593, 0.6436]]) \n",
      "\n",
      "Shape defined: (2, 3)\n",
      "Random Tensor: \n",
      " tensor([[0.7441, 0.4116, 0.5099],\n",
      "        [0.2526, 0.6099, 0.3120]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# directly from data\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(f\"Tensor from data: \\n {x_data} \\n\")\n",
    "\n",
    "# from a NumPy array\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(f\"Tensor from NumPy array: \\n {x_np} \\n\")\n",
    "\n",
    "# from another tensor\n",
    "print(\"Shape given by another tensor:\")\n",
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
    "\n",
    "# with random or constant values\n",
    "shape = (2, 3,)\n",
    "print(\"Shape defined:\", shape)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1.2 Tensor Attributes**\n",
    "\n",
    "We can print information such as the tensor shape, the tensor datatype, and the device on which they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1.3 Tensor Operations**\n",
    "\n",
    "For a full list of available tensor operations check out the corresponding [PyTorch documentation](https://pytorch.org/docs/stable/torch.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n",
    "    print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second column replaced with zeros \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# standard numpy-like indexing and slicing\n",
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0\n",
    "print(f\"Second column replaced with zeros \\n {tensor} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Indexing and Slicing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted submatrix:\n",
      " tensor([[ 1,  2,  3],\n",
      "        [ 5,  6,  7],\n",
      "        [ 9, 10, 11]])\n",
      "Tensor after replacement:\n",
      " tensor([[ 0,  0,  0,  4],\n",
      "        [ 0,  0,  0,  8],\n",
      "        [ 0,  0,  0, 12],\n",
      "        [13, 14, 15, 16]])\n"
     ]
    }
   ],
   "source": [
    "tensor_exercise = torch.tensor([[1, 2, 3, 4],\n",
    "                       [5, 6, 7, 8],\n",
    "                       [9, 10, 11, 12],\n",
    "                       [13, 14, 15, 16]])\n",
    "\n",
    "# Exercise 1: Select the last 3 rows of the first 3 columns and assign them to 'submatrix'\n",
    "# submatrix = ...\n",
    "submatrix = tensor_exercise[:3, :3]\n",
    "print(\"Extracted submatrix:\\n\", submatrix)\n",
    "\n",
    "# Exercise 2: Replace the extracted submatrix with a tensor of zeros in the original tensor\n",
    "# ...\n",
    "tensor_exercise[:3, :3] = torch.zeros(3, 3)\n",
    "print(\"Tensor after replacement:\\n\", tensor_exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated tensor \n",
      " tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# joining tensors (concatenation or stacking)\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(f\"Concatenated tensor \\n {t1} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor.matmul(tensor.T) \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) \n",
      "\n",
      "tensor @ tensor.T \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# multiplying tensors\n",
    "# element-wise product\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")\n",
    "\n",
    "# matrix multiplication\n",
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor before inplace addition \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "Tensor after inplace addition \n",
      " tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inplace operations have a trailing underscore\n",
    "print(f\"Tensor before inplace addition \\n {tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "print(f\"Tensor after inplace addition \\n {tensor} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Element-wise Logarithm, multiplication and calculation of the mean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 1.3863, 3.2958, 5.5452])\n",
      "tensor(2.5568)\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "# Exercise 1: Calculate the element-wise natural logarithm and multiply the result with the tensor itself\n",
    "# log_tensor = ...\n",
    "log_tensor = torch.log(tensor) * tensor\n",
    "print(log_tensor)\n",
    "\n",
    "# Exercise 2: Calculate the mean of the resulting tensor\n",
    "# mean_log = ...\n",
    "mean_log = torch.mean(log_tensor)\n",
    "print(mean_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In-place operations can be problematic when computing derivatives because of a loss of history. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1.4 Bridge with NumPy**\n",
    "\n",
    "Tensors on the CPU and NumPy arrays can share their underlying memory locations, and a change to one will cause the other to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "# tensor to NumPy array\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "# both arrays are modified\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "# NumPy array to tensor\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out=n)\n",
    "# both arrays are modified\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Introduction to torch.autograd\n",
    "\n",
    "``torch.autograd`` is PyTorch's engine for automatic differentiation. It is essential for the training of neural networks.\n",
    "\n",
    "**4.2.1 Differentiation in Autograd**\n",
    "\n",
    "The argument ``required_grad=True`` signals to ``autograd`` that every operation on those tensors should be tracked. This allows ``autograd`` to collect gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If ``a`` and ``b`` are parameters of a neural networks the error function ``Q`` could looks like this:\n",
    "\n",
    "\\begin{align}Q = 3a^3 - b^2\\end{align}\n",
    "\n",
    "For the training of the neural network we need to calculate the gradients with respect to the parameters: \n",
    "\n",
    "\\begin{align}\\frac{\\partial Q}{\\partial a} = 9a^2\\end{align}\n",
    "\n",
    "\\begin{align}\\frac{\\partial Q}{\\partial b} = -2b\\end{align}\n",
    "\n",
    "With ``autograd`` you can calculate those gradients by calling ``.backward()`` on Q. The ``gradient`` argument is used here to specify how much the tensors ``a`` and ``b`` should influence the gradient calculation of ``Q``. By providing ``external_grad`` as the gradient argument, you ensure that both ``a`` and ``b`` are treated as if they contribute equally to the gradient of ``Q`` (both having a weight of 1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)\n",
    "\n",
    "# check if collected gradients are correct\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **torch.autograd**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients of x: tensor([2., 3.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "y = 2 * x[0] + 3 * x[1]\n",
    "\n",
    "# Exercise 1: Calculate gradients\n",
    "# ...\n",
    "y.backward()\n",
    "\n",
    "# Exercise 2: Print gradients of 'x'\n",
    "#print(\"Gradients of x:\", ...)\n",
    "print(\"Gradients of x:\", x.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2.2 Computational Graph**\n",
    "\n",
    "``autograd`` functions by maintaining a record of both data (tensors) and executed operations in a directed acyclic graph (DAG) composed of Function objects. Within this graph structure, the input tensors serve as the starting point (leaves), and the output tensors act as the endpoints (roots). By traversing this graph in a reverse manner, one can automatically compute gradients using the chain rule.\n",
    "\n",
    "- **Forward pass**: ``autograd`` performs operations to compute the resulting tensor and maintains the operation's gradient function in the DAG.\n",
    "\n",
    "- **Backward pass**: ``autograd`` (triggered by calling ``.backward()`` on the DAG root) computes the gradients from each ``.grad_fn``, accumulates them in the corresponding tensor's ``.grad`` attribute, and applies the chain rule to propagate gradients to the leaf tensors.\n",
    "\n",
    "``autograd`` tracks operations for tensors with ``requires_grad=True``, while setting ``requires_grad=False`` excludes them; if any input tensor has ``requires_grad=True``, the output tensor will also require gradients.\n",
    "\n",
    "Note: In PyTorch, DAGs (Directed Acyclic Graphs) are dynamic, and it's important to know that a new graph is built from scratch after each ``.backward()`` call. This flexibility enables the use of control flow statements and the ability to modify the model's shape, size, and operations in each iteration as required.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **DAG**\\\n",
    "Think about the answers before executing the following code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does `a` require gradients?: False\n",
      "Does `b` require gradients?: True\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(f\"Does `a` require gradients?: {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients?: {b.requires_grad}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Neural Networks with PyTorch\n",
    "\n",
    "The ``torch.nn`` package can be used to construct neural networks. ``nn.Module``s contain layers, and a method ``forward(input)`` that returns the ``output``.\n",
    "\n",
    "Typical training procedure for a neural network:\n",
    "1. Define neural network with some learnable parameters (weights)\n",
    "2. Iterate over dataset of inputs\n",
    "3. Process input through network\n",
    "4. Compute the loss (how wrong is the output)\n",
    "5. Backpropagate to calculate the gradient for each of the network's weights\n",
    "6. Update the weights of the network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3.1 Define the network**\n",
    "\n",
    "Define the ``forward`` function. The ``backward`` function is is automatically defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "Number of learnable parameters: 10 \n",
      "\n",
      "torch.Size([6, 1, 5, 5])\n",
      "472\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "# learnable parameters of the model\n",
    "params = list(net.parameters())\n",
    "print(f\"Number of learnable parameters: {len(params)} \\n\")\n",
    "print(params[0].size())  # conv1's .weight\n",
    "\n",
    "print(sum(len(p) for p in net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0086, -0.0776,  0.0718,  0.1250, -0.0378,  0.0303,  0.0305,  0.0819,\n",
      "         -0.0379, -0.1208]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# random input\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero gradient buffers of all parameters and backprops with random gradients\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3.2 Loss Function**\n",
    "\n",
    "The loss function computes a value that estimates how far away the output is from the target. For the full list of available loss functions check out the [PyTorch documentation](https://pytorch.org/docs/stable/nn.html#loss-functions).\n",
    "\n",
    "Example: ``MSELoss`` (mean squared error (squared L2 norm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4233, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3.3 Backprop**\n",
    "\n",
    "To initiate error backpropagation, use ``loss.backward()``, but make sure to clear existing gradients; otherwise, the new gradients will accumulate onto the existing ones. This step is crucial for accurate gradient calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0077, -0.0042,  0.0005, -0.0052,  0.0120, -0.0077])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3.4 Update the weights**\n",
    "\n",
    "A simple update rule is the Stochastic Gradient Descent (SGD):\n",
    "\n",
    "*weight = weight - learning_rate * gradient*\n",
    "\n",
    "The ``torch.optim`` package implements different update rules such as SGB, Nesterov-SGD, Adam, RMSProp, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Neural Networks (1)**\n",
    "1. What is the purpose of the forward method in a PyTorch neural network model?\n",
    "2. How do you define a convolutional layer in PyTorch, and what does the nn.Conv2d module do?\n",
    "3. In the provided code, how many learnable parameters does the neural network model Net have, and how can you access them?\n",
    "4. Explain what happens when you call net.zero_grad() and why it is important.\n",
    "5. What does the optimizer.step() method do, and when is it typically called in the training loop?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "1. The forward method in a PyTorch neural network model defines the forward pass of the network. It specifies how the input data is processed layer by layer to produce the model's output. This method is called when you pass input data to the model.\n",
    "2. To define a convolutional layer in PyTorch, you use the nn.Conv2d module. It represents a 2D convolution operation. You specify the number of input and output channels, as well as the kernel size. It applies the convolution operation to the input data to extract features.\n",
    "3. The Net neural network model has learnable parameters in its layers, such as weights and biases. You can access these parameters using net.parameters(). In the provided code, the number of learnable parameters can be obtained using len(params), where params is a list of the model's parameters.\n",
    "4. When you call net.zero_grad(), it zeroes the gradient buffers of all parameters in the network. This is necessary because gradients accumulate during backpropagation, and you typically want to start with fresh gradients for each mini-batch of training data.\n",
    "5. The optimizer.step() method is used to update the model's parameters based on the gradients computed during backpropagation. It is typically called after calculating the gradients with loss.backward() in the training loop. It performs the parameter updates according to the optimization algorithm (e.g., stochastic gradient descent)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Neural Networks (2)**\\\n",
    "Create  a custom neural network layer called CustomLayer that inherits from nn.Module. The layer should take an input tensor and compute the element-wise square of the input tensor.\n",
    "1. Define the CustomLayer class with a forward method that performs the specified operation.\n",
    "2. Instantiate the CustomLayer.\n",
    "3. Generate a random input tensor with dimensions (1, 3, 4, 4).\n",
    "4. Apply the CustomLayer to the input tensor to calculate the element-wise square.\n",
    "5. Print the input tensor and the resulting output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:\n",
      "tensor([[-0.0540,  1.6705, -0.5522,  0.1150]])\n",
      "\n",
      "Output Tensor (Element-wise Square):\n",
      "tensor([[0.0570, 0.3143, 0.9593, 0.1025]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. Define the CustomLayer class\n",
    "# ...\n",
    "# solution\n",
    "class CustomLayer(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.abs(x) / torch.exp(x)\n",
    "\n",
    "# 2. Instantiate the CustomLayer\n",
    "# ...\n",
    "# solution\n",
    "custom_layer = CustomLayer()\n",
    "\n",
    "# 3. Generate a random input tensor\n",
    "# ...\n",
    "# solution\n",
    "input_tensor = torch.randn(1, 4)\n",
    "\n",
    "# 4. Apply the CustomLayer to the input tensor\n",
    "# ...\n",
    "# solution\n",
    "output_tensor = custom_layer(input_tensor)\n",
    "\n",
    "# 5. Print the input tensor and the resulting output tensor\n",
    "print(\"Input Tensor:\")\n",
    "print(input_tensor)\n",
    "print(\"\\nOutput Tensor (Element-wise Square):\")\n",
    "print(output_tensor)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Training a Classifier (Optional)\n",
    "\n",
    "When dealing with image, text, audio, or video data, you can use standard Python packages to load data into a NumPy array, which can then be converted into a ``torch.*Tensor``.\n",
    "\n",
    "- For images: Packages like Pillow and OpenCV are useful.\n",
    "- For audio: You can use packages such as scipy and librosa.\n",
    "- For text: You have options like raw Python, Cython-based loading, NLTK, and SpaCy.\n",
    "\n",
    "For image-related tasks, ``torchvision`` is a convenient package that provides data loaders for common datasets (e.g., ImageNet, CIFAR10, MNIST) and data transformers for images. This eliminates the need for writing repetitive code.\n",
    "\n",
    "In this tutorial, we'll work with the CIFAR10 dataset, which includes classes like 'airplane,' 'automobile,' 'bird,' 'cat,' 'deer,' 'dog,' 'frog,' 'horse,' 'ship,' and 'truck.' The images in CIFAR-10 are 3-channel color images with dimensions 32x32 pixels.\n",
    "\n",
    "Step:\n",
    "1. Load and normalize CIFAR10 training and test datasets\n",
    "2. Define Convolutional Neural Network (CNN)\n",
    "3. Define loss function\n",
    "4. Train network on training data\n",
    "5. Test network on test data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4.1 Load and normalize CIFAR10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# transform PILImage images\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If running on Windows and you get a BrokenPipeError, try setting the num_worker of torch.utils.data.DataLoader() to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plane truck cat   plane\n"
     ]
    }
   ],
   "source": [
    "# Show example images and labels\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4.2 Define Convolutional Neural Network**\n",
    "\n",
    "Use neural network from section 4.3 and modify it to take 3-channel images (RGB instead of grayscale)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Defining a classifier**\\\n",
    "Fill in the gaps in the following code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # define the first convolutional layer (conv1): 3 input channels, 6 output channels, kernel size: 5\n",
    "        # self.conv1 = ...\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply the first convolutional layer, activation (relu), and pooling\n",
    "        # x = ...\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # apply the first convolutional layer, activation (relu), and pooling\n",
    "        # x = ...\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4.3 Define loss function and optimizer**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Defining a loss function**\\\n",
    "Implement a cross entropy loss function\n",
    "\n",
    "Hint: check out the [PyTorch documentation](https://pytorch.org/docs/stable/nn.html#loss-functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# criterion = ...\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4.4 Train network**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Training a classifier**\\\n",
    "Fill in the gaps in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.175\n",
      "[1,  4000] loss: 1.857\n",
      "[1,  6000] loss: 1.663\n",
      "[1,  8000] loss: 1.550\n",
      "[1, 10000] loss: 1.489\n",
      "[1, 12000] loss: 1.438\n",
      "[2,  2000] loss: 1.369\n",
      "[2,  4000] loss: 1.348\n",
      "[2,  6000] loss: 1.353\n",
      "[2,  8000] loss: 1.315\n",
      "[2, 10000] loss: 1.272\n",
      "[2, 12000] loss: 1.267\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        # ...\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # perform the forward pass\n",
    "        # outputs = ...\n",
    "        outputs = net(inputs)\n",
    "        # calculate the loss ussing the criterion\n",
    "        # loss = ...\n",
    "        loss = criterion(outputs, labels)\n",
    "        # perform the backward pass to compute gradients\n",
    "        # ...\n",
    "        loss.backward()\n",
    "        # update the model's parameters using the optimizer\n",
    "        # ...\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4.5 Test network on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 51 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performance is significantly better than random chance, which would be a 10% accuracy when randomly selecting from 10 classes. It appears that the network has learned something meaningful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
