{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5334cb6-11be-462e-b343-6eb05283df54",
   "metadata": {},
   "source": [
    "# TSFS12 Hand-in exercise 5, extra assignment: Deep Q-learning for highway driving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616400e2-0fc4-4b95-9cfb-efa363a7c7f5",
   "metadata": {},
   "source": [
    "## Initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d2e429-de94-4963-8e8f-dcaf42b1f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import despine\n",
    "\n",
    "import gymnasium\n",
    "import highway_env\n",
    "\n",
    "from dqn_agent import DQN_Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ae551a-823b-4353-9e44-6666867cd3e2",
   "metadata": {},
   "source": [
    "## Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dcc42b-8dbd-4474-b352-c95eeceb9d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_mean(r, M):\n",
    "    \"\"\"Compute sliding mean over M samples for r\"\"\"\n",
    "    n = len(r)\n",
    "    r_M = np.zeros(n)\n",
    "    for k in range(1, n):\n",
    "        r_M[k] = np.mean(r[np.max((0, k - M + 1)):k])\n",
    "    r_M[0] = r_M[1]\n",
    "    return r_M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e736d8db-6e66-46ae-ac4d-6c9f9d2d4ff1",
   "metadata": {},
   "source": [
    "## The simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00d73d7-d0dc-4113-b361-d7a893f615ac",
   "metadata": {},
   "source": [
    "Create the OpenAI gym environment that will be used throughout the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a476c-a0c1-47ce-9490-42183abf00cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gymnasium.make('highway-v0', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b3cee8-a227-4362-b502-29e584c6ef94",
   "metadata": {},
   "source": [
    "To show how the simulator works, run the simulator with random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083dfa16-bd79-4c84-b7e2-680bc6ef3422",
   "metadata": {},
   "outputs": [],
   "source": [
    "s, _ = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    a = env.action_space.sample()\n",
    "    s, r, term, trunc, info = env.step(a)\n",
    "    done = term or trunc\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13011c6d-ef73-42b4-8a3b-ee5032b82f04",
   "metadata": {},
   "source": [
    "Available actions are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee3a628",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.unwrapped.get_wrapper_attr('action_type').ACTIONS_ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f1f497-0968-4a2a-9752-19ec5b7a4918",
   "metadata": {},
   "source": [
    "## Code skeletons for exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ea1c8-7eec-4d15-9439-7386c4ddeb77",
   "metadata": {},
   "source": [
    "### Exercise A.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e51c8f9-2540-443d-93ac-b6b5f11ccef3",
   "metadata": {},
   "source": [
    "### Exercise A.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a317ca78-506b-47c0-b228-00bdcd27287d",
   "metadata": {},
   "source": [
    "Define the neural network model for the functional approximation\n",
    "\n",
    "![QNN](figs/q_nn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c8705-a59b-4e6c-ab9c-7ccc762606b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(25, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 5),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, s):\n",
    "        q = self.layers(self.flatten(s))\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98275937-5ec8-486c-9fdc-0ce1438f61f1",
   "metadata": {},
   "source": [
    "Define an explore-exploit strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9a6a00-5f7a-460c-92c1-0c660218c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_exploit(k, eps):\n",
    "    \"\"\"Return probability for exploration action, a simple strategy\n",
    "    \n",
    "    arguments\n",
    "      k -- episode number\n",
    "      eps -- scalar or a tuple (p0, p1, tau)\n",
    "             if eps is scalar, return constant exploration probability\n",
    "             if eps is tuple, return exponential decay from p0 to p1 with time-constant tao\n",
    "    \"\"\"\n",
    "    if type(eps) == tuple:\n",
    "        p0, p1, tau = eps\n",
    "        p = (p0 - p1) * np.exp(-k / tau) + p1\n",
    "    else:\n",
    "        p = eps\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ebda1c-e1e6-4a5d-a0b1-02bb968e391b",
   "metadata": {},
   "source": [
    "Plot a sample explore/exploit strategy for a 3,000 episode training session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8bc413-1a57-4cfa-a646-a63a360f6b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.arange(0, 3000)\n",
    "_, ax = plt.subplots(num=10, figsize=(10, 7))\n",
    "ax.plot(k, explore_exploit(k, (1.0, 0.05, 200)))\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.set_title(\"Exploration/explotation\")\n",
    "despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec8489e-d00b-45b2-ad5b-c7c9d597b2c0",
   "metadata": {},
   "source": [
    "Load the pre-trained model and display model information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d429dd6a-a271-4b58-a6d8-9ed592375e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = {\n",
    "    \"explore_exploit\": lambda k: explore_exploit(k, (1.0, 0.05, 200)),\n",
    "    \"gamma\": 0.8,\n",
    "    \"batch_size\": 32\n",
    "}\n",
    "\n",
    "pre_trained = DQN_Agent(env, Q_NN, opts)\n",
    "pre_trained.load(\"models/pre-trained/last_model.tar\")\n",
    "pre_trained.eval()  # Put the agent into non-training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e09307-a67a-4c95-ace4-b6aa4656ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315ea8ad-dd41-42c2-b766-9749498a59d8",
   "metadata": {},
   "source": [
    "### Exercise A.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2a69db-e6bd-44dc-93e1-2ea7cabb62a5",
   "metadata": {},
   "source": [
    "Define an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71971123-7eb0-4469-9aef-4b2fa30abcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = {\n",
    "    \"explore_exploit\": lambda k: explore_exploit(k, (1.0, 0.05, 200)),\n",
    "    \"gamma\": 0.8,\n",
    "    \"batch_size\": 32\n",
    "}\n",
    "\n",
    "agent = DQN_Agent(env, Q_NN, opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91777e01-9cae-41a0-892f-8c7e647792de",
   "metadata": {},
   "source": [
    "and train for a few episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90319a19-9602-490a-8405-e5a42d51efa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(10, display=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57df5785-7c6e-46e5-a922-44a36743c75d",
   "metadata": {},
   "source": [
    "Data from training is saved in a dictionary ```agent.stats```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c02f58-f369-418b-b8d7-323de09b8386",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.stats.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa35d8-e04d-43ae-b1f2-0809c13f8dbf",
   "metadata": {},
   "source": [
    "Plot training statistics for pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05829c93-8420-43a2-b3d2-a0fef904ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pre_trained.stats  # Get saved training stats from pre-trained agent\n",
    "_, ax = plt.subplots(1, 2, num=20, figsize=(14, 7))\n",
    "ax[0].plot(sliding_mean(stats[\"episode_rewards\"], 100))\n",
    "ax[0].set_xlabel('Episode')\n",
    "ax[0].set_title('Cumulative reward')\n",
    "despine()\n",
    "\n",
    "ax[1].plot(stats[\"eps\"])\n",
    "ax[1].set_xlabel('Episode')\n",
    "ax[1].set_title('Exploration/exploitation')\n",
    "despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dcd2bc-a3a0-40db-83c1-d21869481850",
   "metadata": {},
   "source": [
    "### Exercise A.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39585235-0525-41c8-baa8-105c895ad958",
   "metadata": {},
   "source": [
    "Generate a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb1196-dd5b-4136-adb8-19c2658afb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s, _ = env.reset()\n",
    "env.render()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1d84bd-48c0-4eed-8020-bdbe392ecaa2",
   "metadata": {},
   "source": [
    "Evaluate the state-value function for the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c23809-6433-4658-844a-702d980d6c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pre_trained.Q(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb92640-3261-4fa4-9064-8c8b53465e30",
   "metadata": {},
   "source": [
    "Define you own observation $s\\in \\mathbb{R}^{5\\times 5}$ and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65d44eb-af5c-4986-9cfc-d9af48edaba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.array([[1, 0, 0, 0, 0],\n",
    "              [1, 0, 0, 0, 0],\n",
    "              [1, 0, 0, 0, 0],\n",
    "              [1, 0, 0, 0, 0],\n",
    "              [1, 0, 0, 0, 0]])\n",
    "print(pre_trained.Q(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fabc04-bfe2-4638-a9eb-5ae8cbbcdc01",
   "metadata": {},
   "source": [
    "Run the pre-trained model (try your own also)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d53ee7b-c671-4cea-bd61-d7fdde43d66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random seed between 0 and 1000; save for later if you find an interesting scenario\n",
    "seed = np.random.randint(0, 1000)\n",
    "# env.config[\"vehicles_density\"] = 1.25  # Increase density, default = 1.0\n",
    "\n",
    "s, _ = env.reset(seed=seed)\n",
    "done = False\n",
    "tot_reward = 0\n",
    "while not done:\n",
    "    a = pre_trained.act(s)\n",
    "    s, r, term, trunc, info = env.step(a)\n",
    "    done = term or trunc\n",
    "    tot_reward += r\n",
    "    env.render()\n",
    "env.close()\n",
    "print(f\"Finished episode with total reward {tot_reward:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a2a0f5-070f-4761-9a53-90ef881bf031",
   "metadata": {},
   "source": [
    "### Exercise A.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce504256-c892-4b44-a193-d27b4132f3fd",
   "metadata": {},
   "source": [
    "This exercise evaluates how the value faunction $V(s)$ connects to the action-value functio $Q(s, a)$ in a scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb418df-2469-4e6c-ac6d-841a496c0242",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = np.random.randint(0, 1000)\n",
    "\n",
    "s, _ = env.reset(seed=seed)\n",
    "done = False\n",
    "tot_reward = 0\n",
    "s_scenario = []  # List with all states during sceneario execution\n",
    "while not done:\n",
    "    s_scenario.append(s)\n",
    "\n",
    "    a = pre_trained.act(s)\n",
    "    s, r, term, trunc, info = env.step(a)\n",
    "    done = term or trunc\n",
    "    env.render()\n",
    "\n",
    "    tot_reward += r\n",
    "    \n",
    "env.close()\n",
    "print(f\"Finished episode with total reward {tot_reward:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67d1da0",
   "metadata": {},
   "source": [
    "Use the list of all states visited during the scenario, ```s_scenrio``` to compute and plot ab estimate for ```V(s)``` for the states visited during the scenario. Also, determine an upper bound for the true ```V(s)```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4165ac-0d62-4b8f-9f43-369cdba0def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = None  # Your code here\n",
    "V_uppper_bound = 0  # Your code here\n",
    "\n",
    "_, ax = plt.subplots(num=30, clear=True, figsize=(12, 8))\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805f8f41-7e9d-4271-8192-275040cc4192",
   "metadata": {},
   "source": [
    "### Exercise A.6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "0fb71cafd0ca8f610c491b528cef890e5cdcec0f7324617a6e888791aac5a9e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
