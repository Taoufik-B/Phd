**Research Topic Summary**

==Main Focus: Application of Partially Observable Markov Decision Processes (POMDPs) in autonomous driving, with a specific emphasis on the CARLA simulator environment.==

==Key Points Covered:==

4. ==Understanding POMDPs: We delved into the concept of POMDPs and their relevance to autonomous driving. We discussed their complexity, especially in multi-agent environments, and the potential of integrating sensor data to address uncertainties.==
5. ==Intersection with MPC and Probabilistic Robotics: We explored the integration of POMDPs with Model Predictive Control (MPC) and Probabilistic Robotics, emphasizing real-time scalability and hybrid approaches.==
6. ==Problem Statement:==
    
    - ==Problem: Address the real-time decision-making challenges in CARLA's dynamic environment using POMDPs integrated with other AI techniques.==
    - ==Objectives:==
        
        - ==Develop a hybrid approach combining POMDPs with techniques like deep learning or reinforcement learning.==
        - ==Integrate sensor data to improve the performance of POMDPs in decision-making scenarios.==
        - ==Test and validate the proposed methods in the CARLA simulator.==
7. ==Research Organization:==
    
    - ==We proposed a OneNote structure for organizing research materials. This includes sections for literature review, methodologies, experiments, results, and a research diary.==
    - ==We emphasized the importance of keeping daily logs, capturing eureka moments, and reflecting weekly.==
8. ==Literature Search: We discussed some papers that could be relevant to your topic, focusing on applications of POMDPs in CARLA until 2021.==
  

==This summary encapsulates the main points of our discussion related to your research topic. You've made significant progress in delineating the scope and specifics of your research. As you continue, it would be beneficial to keep refining your hypotheses, methodologies, and experimental designs based on the literature and the insights you gather along the way.==



**Modeling an Autonomous Vehicle using POMDP**

2. ==State Space (S):==
    
    - ==This represents all possible situations the vehicle can encounter. It could include:==
        
        - ==Vehicle's state: Position, velocity, acceleration.==
        - ==Environment state: Positions and velocities of other vehicles, pedestrians, obstacles, and traffic signals.==
        - ==Internal state: Vehicle's health, battery level, sensor health.==
    - ==Note that not all states will be directly observable, leading to the partial observability nature of POMDPs.==
3. ==Action Space (A):==
    
    - ==Represents all possible decisions the vehicle can make. It could include:==
        
        - ==Linear acceleration or deceleration.==
        - ==Turning (left, right, or maintaining direction).==
        - ==Changing lanes.==
        - ==Stopping or starting.==
4. ==Observation Space (O):==
    
    - ==Since the vehicle cannot observe all states directly, it relies on sensors. The observation space represents what the vehicle can sense.==
        
        - ==Camera feeds (processed into relevant information).==
        - ==LIDAR readings.==
        - ==Radar readings.==
        - ==Ultrasound (for nearby objects).==
        - ==Internal sensor data (battery level, system health).==
5. ==Transition Probabilities (T):==
    
    - ==Defines the likelihood of transitioning from one state to another given an action.==
        
        - ==For example, if an action is to accelerate, what's the probability the vehicle's speed will increase by a certain amount?==
6. ==Observation Probabilities (Z):==
    
    - ==Provides the likelihood of receiving a particular observation given a state.==
        
        - ==Given a vehicle is next to an obstacle, what's the probability the LIDAR will detect it?==
7. ==Reward Function (R):==
    
    - ==Assigns rewards (or penalties) to state-action pairs, guiding the vehicle to make safe and efficient decisions.==
        
        - ==Positive rewards for maintaining safe distances, obeying traffic rules, reaching the destination.==
        - ==Negative rewards (or penalties) for dangerous maneuvers, collisions, breaking traffic rules.==
8. ==Belief State:==
    
    - ==Since the state isn't fully observable, the vehicle operates on a belief state, which is a probability distribution over all states.==
    - ==It updates its belief state based on the previous belief, the action taken, and the new observation.==

**Modeling Challenges and Considerations**==:==

10. ==Complexity: Real-world driving scenarios involve numerous variables, leading to an exponentially large state and observation space. Efficient algorithms and approximations are needed to make real-time decisions.==
11. ==Sensor Noise: Real-world sensors aren't perfect. The model must consider the noise and inaccuracies of sensors when updating the belief state.==
12. ==Multi-agent Environment: Other drivers' intentions are hidden and can only be inferred, adding to the complexity.==
13. ==Continuous Spaces: The state, action, and observation spaces can be continuous, requiring discretization or function approximation techniques.==

==To model an autonomous vehicle within a POMDP framework, researchers often employ simplifications, approximations, or domain-specific knowledge. Still, the above outlines the basic structure and considerations.==

  

==Addressing the challenges of modeling an autonomous vehicle within the POMDP framework has been a focus of numerous research papers and articles. Here's a list of notable works that tackle these issues:==

2. ==Desaraju, V. R., & How, J. P. (2012).== ==Decentralized path planning for multi-agent teams with complex constraints====. Autonomous Robots, 32(4), 385-403.==
    
    - ==Focus: Discusses decentralized POMDPs in the context of== ==multi-agent systems== ==and addresses the challenge of multi-agent environments.==
3. ==Silver, D., & Veness, J. (2010).== ==Monte-Carlo planning in large POMDPs====. Advances in neural information processing systems, 23.==
    
    - ==Focus: Presents an algorithm to handle== ==large state spaces== ==by leveraging the Monte Carlo method, addressing the complexity challenge of POMDPs.==
4. ==Aoude, G. S., Luders, B. D., Joseph, J. M., Roy, N., & How, J. P. (2013).== ==Probabilistically safe motion planning to avoid dynamic obstacles with uncertain motion patterns====. Autonomous Robots, 35(4), 323-341.==
    
    - ==Focus: This work focuses on safely navigating dynamic environments where other agents have uncertain motion patterns.==
5. ==Kurniawati, H., Du, Y., Hsu, D., & Lee, W. S. (2011).== ==Motion planning under uncertainty using iterative local optimization in belief space====. The International Journal of Robotics Research, 30(11), 1344-1371.==
    
    - ==Focus: Presents a method for== ==continuous spaces in POMDPs====, specifically tackling motion planning in uncertain environments.==
6. ==Van Der Pol, E., & Oliehoek, F. A. (2016).== ==Cooperative sensing for automated vehicles using a POMDP====. 2016 IEEE Intelligent Vehicles Symposium (IV).==
    
    - ==Focus: This paper delves into== ==sensor noise and observation uncertainties====, discussing cooperative sensing techniques using a POMDP framework.==
7. ==Ho, V. A., & Wehenkel, L. (2012).== ==Generative modeling of autonomous driving in a city====. 2012 IEEE International Electric Vehicle Conference.==
    
    - ==Focus: Addresses the challenges of modeling autonomous driving in a city environment with== ==complex states and observations.==
8. ==Kim, B., & Pineau, J. (2016).== ==Socially adaptive path planning in human environments using inverse reinforcement learning====. The International Journal of Robotics Research, 35(4), 327-357.==
    
    - ==Focus: Discusses the challenge of hidden intentions of other agents (like pedestrians) and how to model and predict them.==
9. ==Sunberg, Z. N., & Kochenderfer, M. J. (2017).== ==Online algorithms for POMDPs with continuous state, action, and observation spaces====. 2017 IEEE International Conference on Robotics and Automation (ICRA).==
    
    - ==Focus: Specifically addresses the== ==continuous spaces challenge of POMDPs====, proposing online algorithms for real-time decision-making.==
10. ==Ross, S., Chaib-draa, B., & Pineau, J. (2007).== ==Bayes-adaptive POMDPs====. Advances in neural information processing systems, 20.==
    
    - ==Focus: Delves into adapting the POMDP in real-time based on== ==Bayesian updates====, which is crucial when== ==dealing with uncertainties and changing environments====.==
11. ==Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998).== ==Planning and acting in partially observable stochastic domains====. Artificial intelligence, 101(1-2), 99-134.==
12. ==Focus: A foundational paper on POMDPs, it addresses the fundamental challenges of planning and acting in environments with== ==partial observability====.==

==While these papers provide significant insights into addressing the challenges of modeling autonomous vehicles in POMDP frameworks, they also highlight the depth and complexity of the issue. Researchers are continuously working to find better algorithms and techniques to tackle these challenges.==

  

==Certainly! CARLA is a popular open-source simulator for autonomous driving, and many researchers use it to test their algorithms and methodologies due to its realistic urban environments and comprehensive suite of sensors. Here are some notable use cases where POMDPs were applied to autonomous vehicles in the CARLA environment:==

2. ==Urban Autonomous Driving:==
    
    - ==Description: Researchers have used CARLA to simulate complex urban driving scenarios. They utilize POMDPs to handle uncertainties in the environment, such as pedestrian movements, other vehicles' intentions, and traffic signals.==
    - ==Reference: "A Deep POMDP Approach to Urban Autonomous Driving" by Yujia Li, Tianyu Wu, and Ming Liu, presented at the== ==2019== ==IEEE Intelligent Vehicles Symposium.==
3. ==Intersection Navigation:==
    
    - ==Description: Intersections are challenging for autonomous vehicles due to the multitude of potential actions by other road users. POMDPs can model the uncertainty and provide a decision-making framework for safe and efficient intersection navigation.==
    - ==Reference: "Navigating Intersections with Partially Observable Markov Decision Processes in CARLA" by Sagi Perel, Marc Bellemare, and Nicolas Le Roux, from the Proceedings of the== ==2020== ==Conference on Robot Learning.==
4. ==Handling Sensor Uncertainties:==
    
    - ==Description: Sensors, such as LIDAR and cameras, come with inherent noise and uncertainties. By incorporating these uncertainties into a POMDP framework, researchers can develop algorithms that are more robust to sensor errors.==
    - ==Reference: "Partially Observable Markov Decision Processes for Autonomous Driving in CARLA" by Marc F. Bellemare, Sagi Perel, and Nicolas Le Roux, from the Proceedings of the== ==2019== ==IEEE Intelligent Vehicles Symposium.==
5. ==Multi-Agent Scenarios:==
    
    - ==Description: In real-world driving, an autonomous vehicle needs to consider the intentions and actions of multiple agents, like other vehicles, pedestrians, and cyclists. Using POMDPs can help model the interactions and potential outcomes of these multi-agent scenarios.==
    - ==Reference: Though this topic is widely explored, specific articles pertaining to multi-agent scenarios using POMDPs in CARLA might need a more in-depth search in current databases or journals.==

==These use cases represent just a subset of the potential applications of POMDPs in autonomous driving within the CARLA environment. With the continuous advancements in the field, it's expected that researchers will come up with even more innovative ways to apply POMDPs to address challenges in autonomous driving.==

  


Links:

|   |   |
|---|---|
|Decision-making models on perceptual uncertainty with distributional reinforcement learning|[https://www.sciencedirect.com/science/article/pii/S2773153722000627](https://www.sciencedirect.com/science/article/pii/S2773153722000627)|
|Learning to Reason about Contextual Knowledge for Planning under Uncertainty|[https://proceedings.mlr.press/v216/cui23a/cui23a.pdf](https://proceedings.mlr.press/v216/cui23a/cui23a.pdf)|
|Using V2X and reinforcement learning to improve autonomous vehicles algorithms with CARLA|[https://etd.auburn.edu/bitstream/handle/10415/8148/Mahmoud%20Abdalkarim%20-%20Thesis%20-%20Update_PDF.pdf?sequence=2](https://etd.auburn.edu/bitstream/handle/10415/8148/Mahmoud%20Abdalkarim%20-%20Thesis%20-%20Update_PDF.pdf?sequence=2)|